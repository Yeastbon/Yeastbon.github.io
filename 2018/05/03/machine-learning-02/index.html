<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Coursera机器学习笔记02 | 一个还没有想好叫什么名字的博客</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/8.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.3.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Coursera机器学习笔记02</h1><a id="logo" href="/.">一个还没有想好叫什么名字的博客</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Coursera机器学习笔记02</h1><div class="post-meta">May 3, 2018</div><div class="post-content"><h2 id="多变量的线性回归"><a href="#多变量的线性回归" class="headerlink" title="多变量的线性回归"></a>多变量的线性回归</h2><p>上周已经接触到了只有一个自变量的线性回归，同样以预估房价为例，一个房子的售价不只受到房屋面积的影响，还与房屋的户型，层数以及新旧等有关，因此我们在预估房价的时候，应该考虑更多的变量，我们把这样的变量称为feature。这样我们可以使用一个$m\times n$的矩阵$X$储存所有的feature数据。这里$m$表示训练集的数量，$n$表示feature特征的数量 ，如我们的例子，我提到了面积、户型、层数和新旧，因此$n=4$。在这里可能会遇到各种特征的选择的问题，例如，训练集中同时有房屋的长度和宽度，我们可以将来两个特征相乘来得到新的面积特征来取代长度和宽度两个特征。尽可能的减少特征的数量对模型的建立有很大的好处。</p>
<h4 id="hypothesis"><a href="#hypothesis" class="headerlink" title="hypothesis"></a>hypothesis</h4><p>由于特征的增多，我们把$h_\theta(x)$改写为：<br>$$<br>h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+{\cdots}+\theta_nx_n<br>$$</p>
<p>为了结构的整齐以及后面计算的方便，令每个训练集的$x_0$都等于1，这样矩阵$X$可以被表示为$ X=\left[ \begin{smallmatrix} x_0&amp;x_1&amp;\cdots&amp;x_n \end{smallmatrix} \right]$，或者$X=\left[\begin{smallmatrix} x^{(1)}\\x^{(2)}\\\vdots\\x^{(m)} \end{smallmatrix}\right]$，因此$X$变成了是一个$m\times(n+1)$的矩阵，在原来的$m\times n$的$X$矩阵的基础上在最左侧加上一行全为1的列向量，同样的$\theta_0\cdots\theta_n$也可以表示为$\theta=\left[\begin{smallmatrix}\theta_0\\\theta_1\\\vdots\\\theta_n\end{smallmatrix}\right]$，因此$h_\theta(x)$可以写成：<br>$$<br>\begin{equation}\begin{split}<br>h_\theta(x)&amp;=&amp;\theta_0x_0+\theta_1x_1+\theta_2x_2+\cdots+\theta_nx_n\<br>&amp;=&amp;X\cdot\theta<br>\end{split}\end{equation}<br>$$</p>
<h4 id="cost-function"><a href="#cost-function" class="headerlink" title="cost function"></a>cost function</h4><p>$$<br>J(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2<br>$$</p>
<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>梯度下降的算法没有发生任何变化，<br>$$<br>\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)<br>$$<br>$j=0\cdots n$，将等式中的偏导计算出来，即可得到：<br>$$<br>\theta_j:=\theta_j-\alpha\cdot\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot x_j^{(i)}<br>$$<br>其中，$x_0=\left( \begin{smallmatrix}1\\1\\\vdots\\1 \end{smallmatrix}\right)$，正好可以满足$\theta_0$的更新算法。同样的，我们在执行算法的过程中，还是要注意，需要同时更新所有的$\theta$值。</p>
<h4 id="feature-scaling-amp-amp-mean-normalization——特征放缩与均值归一化"><a href="#feature-scaling-amp-amp-mean-normalization——特征放缩与均值归一化" class="headerlink" title="feature scaling &amp;&amp; mean normalization——特征放缩与均值归一化"></a>feature scaling &amp;&amp; mean normalization——特征放缩与均值归一化</h4><p>当存在多个feature的时候，由于每个feature的范围差距比较大，会导致$\theta$在范围较小的维度上下降很快，在范围较大的维度上下降很慢，并且有可能会在最小值(optimum)附近震荡。出于以上原因我们可以将数据进行特征放缩和均值归一化，即：<br>$$<br>x_i:=\frac{x_i-\mu_i}{s_i}<br>$$<br>其中，$\mu_i$表示第$i$个特征的均值，$s_i$表示第$i$个特征的标准差(或者也可以使用范围，即该特征的最大值与最小值之差)。</p>
<p>将数据进行标准化之后，每个数据的值都可以保持在$-1\sim1$或$-0.5\sim0.5$之间($x_0$不需要进行特征放缩和归一化)。另外，这个过程的目的仅限于使梯度下降更快，并且让迭代的次数尽可能的减少，因此并不需要过于拘泥于方法和精度，只要大概完成即可。</p>
<h4 id="J-theta-与迭代次数的图像"><a href="#J-theta-与迭代次数的图像" class="headerlink" title="$J(\theta)$与迭代次数的图像"></a>$J(\theta)$与迭代次数的图像</h4><p>在调试函数的过程中，可以做出$J(\theta)$和迭代次数的图像，通过图像可以看到$J(\theta)$是否在每一次的迭代之后都减小了，并且可以通过图像的斜率来判断算法是否已经收敛。</p>
<p>通过这个思路，我们其实可以完成自动判断收敛的算法(Automatic convergence)：如果在某一次的迭代中$J(\theta)$的减小量小于某个$\epsilon$(例如：$10^{-3}$)就可以认为算法已经收敛了，但是这样做的一个缺点是我们不太容易确定出合适的$\epsilon$    ，因此大多数的时候，我们还是用图像的办法确定算法是否已经收敛了。</p>
<p>该图像还可以给我们提供的信息是，可以通过图像选取一个合适的$\alpha$。如果图像下降的太慢，可以得出$\alpha$太小的结论，而如果图像出现了上下震荡，甚至持续上升的情况，这证明$\alpha$太大了。这样我们就可以通过图像来动态的调整$\alpha$的值。</p>
<p>经验表明，我们可以从一个较小的值逐步变大，选取一个迭代次数最小，又能保证算法收敛的合适的$\alpha$值。例如，首先令$\alpha=0.0001$，然后以一个3倍左右的倍数逐渐放大$\alpha$，即：$0.0001\to0.0003\to0.001\to0.003\to0.01\to0.03\cdots$</p>
<p>总结一下：</p>
<ul>
<li>$ \alpha$如果太小，会使算法收敛太慢</li>
<li>$\alpha$如果太大，可能每次迭代不会都使$J(\theta)$减小，甚至也有可能使算法发散，当然，也有可能算法可以收敛，但是收敛很慢。</li>
</ul>
<h2 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h2><p>在大多数情况下，只用一次函数可能不能很准确的拟合训练集的数据，在这样的情况下，我们可以引入多项式回归。<br>$$<br>\begin{equation}\begin{split}<br>&amp;二次函数：h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x^2_1\\<br>&amp;三次函数：h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x^2_1+\theta_3x_1^3\\<br>&amp;根式函数：h_\theta(x)=\theta_0+\theta_1x_1+\theta_2\sqrt{x_1}\\<br>\end{split}\end{equation}<br>$$</p>
<p>以三次函数为例，可以令$x_1^2=x_2$，$x_1^3=x_3$，这样$h_\theta(x)$就可以表示为：$h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_3$，这样问题又回到了线性回归的问题。</p>
<p>在多项式回归问题中，特征放缩和均值归一化显得尤其重要，因为对一组数据乘方和立方，对数据的范围改变极大，必须使用特征放缩和均值归一，使得数据的范围大体一致。</p>
<h2 id="正规方程——Normal-Equation"><a href="#正规方程——Normal-Equation" class="headerlink" title="正规方程——Normal Equation"></a>正规方程——Normal Equation</h2><p>正规方程提供了一种方法，可以用训练集的数据，直接求出$\theta$而不需要进行迭代运算。我们知道，一个函数可以通过求其各个自变量偏导数，并令偏导数为零的办法求出每一个自变量的值。但是对于$J(\theta)$来说，自变量可能很多，这样的求解方法似乎不是十分的可行，但是这也不失为一种合理的思路。</p>
<p>正规方程：<br>$$<br>\theta=(X^TX)^{-1}X^Ty<br>$$<br>其中，$X$同样使前文提到的我们构造的$m\times (n+1)$的矩阵，$y$为$m$个元素的列向量。</p>
<p>由于采用正规方程的方法，不需要进行迭代计算，因此不需要特征放缩和均值归一化的操作。</p>
<h4 id="梯度下降与正规方程的比较"><a href="#梯度下降与正规方程的比较" class="headerlink" title="梯度下降与正规方程的比较"></a>梯度下降与正规方程的比较</h4><p>梯度下降方法与正规方程比起来，有两个明显的缺点：</p>
<ul>
<li>需要确定$\alpha$，这个过程需要进行人为的控制</li>
<li>需要进行迭代计算，计算量很大</li>
</ul>
<p>综上所述，正规方程在处理线性回归问题时有其明显的优势。</p>
<p>但是正规方程也有一定的缺点：$X^TX$是一个$(n+1)\times(n+1)$的矩阵，而求逆的复杂度为$O(n^3)$，因此对于特征太多的训练集(即n太大)，正规方程的方法显得太慢了，而反过来梯度下降的复杂度为$O(n^2)$，因此当n很大时，应该选择梯度下降的方法，可以大概以10,000为标准，当$n\gt10,000$时采用梯度下降的算法，反之，采用正规方程的方法。另外，在之后要学习的很多更复杂的算法，正规方程就不能应用了，也就是说正规方程的方法只能应用与线性回归的计算。而梯度下降的方法在之后的很多复杂算法中，仍然有很重要的应用。</p>
<h4 id="X-TX-的可逆性"><a href="#X-TX-的可逆性" class="headerlink" title="$X^TX$的可逆性"></a>$X^TX$的可逆性</h4><p>$X^TX$是一个$(n+1)\times(n+1)$的矩阵，在正规方程的计算中，需要计算其逆矩阵，但是如果$X^TX$不可逆，这种情况在Octave/Matlab的程序中，处理起来也十分的容易。Octave程序中有两个求拟的函数，分别为$inv()$和$pinv()$，$pinv()$可以对非奇异矩阵进行类似与求逆的操作，并且返回一个与逆矩阵性质及其相似的矩阵。从函数的效率上，$pin()$的效率比$pin()$要高。</p>
<p>另外，我们可以分析一下造成$X^TX$不可逆的情况，$X^TX$不可逆说明其线性相关，第一种情况就是存在多余的特征，例如训练集中有两个特征分别用米和英尺为单位来表示房屋的长度，这样的训练集一定是线性相关的，当然这种情况的处理也很容易，只需要删除其一即可。</p>
<p>另外一种情况是，特征太多了，可以用$n\ge m$来判断这种情况，针对这种情况，也可以采用删除特征的办法来解决，另外还可以使用正则化(regularization)的技术来解决这个问题，这项技术会在以后学到。</p>
<h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p>在前一篇文章中已经提到了，在实际编写机器学习的程序时，如果使用线性代数的库函数进行操作，将大大提高计算效率和代码的行数。下面将用两个例子来说明使用库函数的优势。</p>
<h4 id="假设函数-h-theta-x"><a href="#假设函数-h-theta-x" class="headerlink" title="假设函数$h_\theta(x)$"></a>假设函数$h_\theta(x)$</h4><p>以假设函数为例，<br>$$<br>\begin{equation}\begin{split}<br>h_\theta(x)&amp;=\sum_{j=0}^n\theta_jx_j\\<br>&amp;=\theta^Tx<br>\end{split}\end{equation}<br>$$<br>使用for循环的方法计算假设函数，代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">prediction = 0.0;</span><br><span class="line">for j = 1:n+1,</span><br><span class="line">	prediction = prediction + theta(j) * x(y)</span><br><span class="line">end;</span><br></pre></td></tr></table></figure>
<p>而使用向量化的技巧，只需要一行代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = theta&apos; * x;</span><br></pre></td></tr></table></figure>
<h4 id="梯度下降算法的向量化"><a href="#梯度下降算法的向量化" class="headerlink" title="梯度下降算法的向量化"></a>梯度下降算法的向量化</h4><p>梯度下降的算法如下：<br>$$<br>\begin{equation}\begin{split}<br>\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}\\<br>\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_1^{(i)}\\<br>\end{split}\end{equation}<br>$$<br>不使用向量化的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">temp1 = theta(1)-alpha*sum((X*theta-y).*X(:,1))/m;</span><br><span class="line">temp2 = theta(2)-alpha*sum((X*theta-y).*X(:,2))/m;</span><br><span class="line">theta(1)=temp1;</span><br><span class="line">theta(2)=temp2;</span><br></pre></td></tr></table></figure>
<p>而使用向量化，同样我们仍然可以使用一行代码来解决：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">theta = theta - (alpha*sum((X*theta-y).*X)/m)&apos;;</span><br></pre></td></tr></table></figure>
<p>再次强调，这样做的好处不仅仅是代码量的减少，而且对于大量数据的情况下，对于计算效率的提高也是十分显著的。</p>
</div><div class="tags"></div><div class="post-nav"><a class="next" href="/2018/04/20/machine_learning_01/">Coursera机器学习笔记01</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="http://yoursite.com"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2018/05/03/machine-learning-02/">Coursera机器学习笔记02</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/20/machine_learning_01/">Coursera机器学习笔记01</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/04/19/hello/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> 友情链接</i></div><ul></ul><a href="http://www.example1.com/" title="site-name1" target="_blank">site-name1</a><ul></ul><a href="http://www.example2.com/" title="site-name2" target="_blank">site-name2</a><ul></ul><a href="http://www.example3.com/" title="site-name3" target="_blank">site-name3</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">一个还没有想好叫什么名字的博客.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/fancybox/3.2.5/jquery.fancybox.min.css"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>